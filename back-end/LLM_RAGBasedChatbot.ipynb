{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -qqq tiktoken\n",
        "!pip install -qqq pinecone-client\n",
        "!pip install -qqq openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I1QPMJGK1hCK",
        "outputId": "ad2fc168-dccf-4df4-d818-5c4aa754abaf"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m32.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.8/244.8 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.4/85.4 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m386.9/386.9 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.0/78.0 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m325.2/325.2 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install -qU langchain-openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wCyoYKFv1jIQ",
        "outputId": "c15483db-2c3e-42b9-dc3e-c2e0758ba004"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/50.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.6/50.6 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.9/49.9 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m408.0/408.0 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.9/296.9 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.5/144.5 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "from google.colab import userdata\n",
        "OPENAI_API_KEY=userdata.get('openai-key')\n",
        "PINECONE_API_KEY=userdata.get('pinecone-key')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "LHVfiCgD2A1F"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['OPENAI_API_KEY'] = OPENAI_API_KEY\n",
        "os.environ['PINECONE_API_KEY'] = PINECONE_API_KEY"
      ],
      "metadata": {
        "id": "T19MeOB_2a6i"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Load the CSV Data Using Pandas"
      ],
      "metadata": {
        "id": "HHvQkQte2eLX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the CSV file (adjust the path as needed)\n",
        "data = pd.read_csv('/content/card_related_dataset.csv')\n",
        "# Display the first few rows to verify the data\n",
        "print(data.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BDQwihyG2im3",
        "outputId": "052189a7-6d46-4a83-b6b1-8c861110a2a6"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                         instruction         intent  \\\n",
            "0   I would like to acivate a card, can you help me?  activate_card   \n",
            "1  I have to activate an Visa online, how can I d...  activate_card   \n",
            "2        I'd like to actiate a card where do i do it  activate_card   \n",
            "3  I'd likke to activate a visa on mobile i need ...  activate_card   \n",
            "4  I would ilke to activate a credit card online,...  activate_card   \n",
            "\n",
            "                                            response  \n",
            "0  I'm here to assist you with that! Activating y...  \n",
            "1  I'm here to assist you with activating your {{...  \n",
            "2  I can help you with that! Activating your card...  \n",
            "3  I'm here to assist you with activating your {{...  \n",
            "4  I'm here to assist you with activating your cr...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Generating Embeddings with langchain_openai"
      ],
      "metadata": {
        "id": "pfsuisZe3AqU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import OpenAIEmbeddings\n",
        "import time\n",
        "\n",
        "# Initialize the embedding model\n",
        "embed_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "\n",
        "# Function to generate embeddings in batches\n",
        "def generate_embeddings_in_batches(documents, batch_size=100):\n",
        "    embeddings = []\n",
        "    for i in range(0, len(documents), batch_size):\n",
        "        batch = documents[i:i+batch_size]\n",
        "        try:\n",
        "            # Generate embeddings for the batch and extend to the main list\n",
        "            batch_embeddings = embed_model.embed_documents(batch)\n",
        "            embeddings.extend(batch_embeddings)\n",
        "            print(f\"Processed batch {i//batch_size + 1}/{len(documents)//batch_size + 1}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error in batch {i//batch_size + 1}: {e}\")\n",
        "            time.sleep(5)  # Wait a bit before retrying in case of API rate limits\n",
        "    return embeddings\n",
        "\n",
        "# Generate embeddings for each instruction in the CSV in batches\n",
        "data['instruction_embedding'] = generate_embeddings_in_batches(data['instruction'].tolist())\n",
        "\n",
        "# Verify the dimension of the embeddings\n",
        "print(f\"Embedding dimension: {len(data['instruction_embedding'][0])}\")\n",
        "print(f\"Number of embeddings generated: {len(data['instruction_embedding'])}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "04aKbYDK3bC7",
        "outputId": "438db02c-855f-493f-e404-9fbc908a8fbb"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed batch 1/256\n",
            "Processed batch 2/256\n",
            "Processed batch 3/256\n",
            "Processed batch 4/256\n",
            "Processed batch 5/256\n",
            "Processed batch 6/256\n",
            "Processed batch 7/256\n",
            "Processed batch 8/256\n",
            "Processed batch 9/256\n",
            "Processed batch 10/256\n",
            "Processed batch 11/256\n",
            "Processed batch 12/256\n",
            "Processed batch 13/256\n",
            "Processed batch 14/256\n",
            "Processed batch 15/256\n",
            "Processed batch 16/256\n",
            "Processed batch 17/256\n",
            "Processed batch 18/256\n",
            "Processed batch 19/256\n",
            "Processed batch 20/256\n",
            "Processed batch 21/256\n",
            "Processed batch 22/256\n",
            "Processed batch 23/256\n",
            "Processed batch 24/256\n",
            "Processed batch 25/256\n",
            "Processed batch 26/256\n",
            "Processed batch 27/256\n",
            "Processed batch 28/256\n",
            "Processed batch 29/256\n",
            "Processed batch 30/256\n",
            "Processed batch 31/256\n",
            "Processed batch 32/256\n",
            "Processed batch 33/256\n",
            "Processed batch 34/256\n",
            "Processed batch 35/256\n",
            "Processed batch 36/256\n",
            "Processed batch 37/256\n",
            "Processed batch 38/256\n",
            "Processed batch 39/256\n",
            "Processed batch 40/256\n",
            "Processed batch 41/256\n",
            "Processed batch 42/256\n",
            "Processed batch 43/256\n",
            "Processed batch 44/256\n",
            "Processed batch 45/256\n",
            "Processed batch 46/256\n",
            "Processed batch 47/256\n",
            "Processed batch 48/256\n",
            "Processed batch 49/256\n",
            "Processed batch 50/256\n",
            "Processed batch 51/256\n",
            "Processed batch 52/256\n",
            "Processed batch 53/256\n",
            "Processed batch 54/256\n",
            "Processed batch 55/256\n",
            "Processed batch 56/256\n",
            "Processed batch 57/256\n",
            "Processed batch 58/256\n",
            "Processed batch 59/256\n",
            "Processed batch 60/256\n",
            "Processed batch 61/256\n",
            "Processed batch 62/256\n",
            "Processed batch 63/256\n",
            "Processed batch 64/256\n",
            "Processed batch 65/256\n",
            "Processed batch 66/256\n",
            "Processed batch 67/256\n",
            "Processed batch 68/256\n",
            "Processed batch 69/256\n",
            "Processed batch 70/256\n",
            "Processed batch 71/256\n",
            "Processed batch 72/256\n",
            "Processed batch 73/256\n",
            "Processed batch 74/256\n",
            "Processed batch 75/256\n",
            "Processed batch 76/256\n",
            "Processed batch 77/256\n",
            "Processed batch 78/256\n",
            "Processed batch 79/256\n",
            "Processed batch 80/256\n",
            "Processed batch 81/256\n",
            "Processed batch 82/256\n",
            "Processed batch 83/256\n",
            "Processed batch 84/256\n",
            "Processed batch 85/256\n",
            "Processed batch 86/256\n",
            "Processed batch 87/256\n",
            "Processed batch 88/256\n",
            "Processed batch 89/256\n",
            "Processed batch 90/256\n",
            "Processed batch 91/256\n",
            "Processed batch 92/256\n",
            "Processed batch 93/256\n",
            "Processed batch 94/256\n",
            "Processed batch 95/256\n",
            "Processed batch 96/256\n",
            "Processed batch 97/256\n",
            "Processed batch 98/256\n",
            "Processed batch 99/256\n",
            "Processed batch 100/256\n",
            "Processed batch 101/256\n",
            "Processed batch 102/256\n",
            "Processed batch 103/256\n",
            "Processed batch 104/256\n",
            "Processed batch 105/256\n",
            "Processed batch 106/256\n",
            "Processed batch 107/256\n",
            "Processed batch 108/256\n",
            "Processed batch 109/256\n",
            "Processed batch 110/256\n",
            "Processed batch 111/256\n",
            "Processed batch 112/256\n",
            "Processed batch 113/256\n",
            "Processed batch 114/256\n",
            "Processed batch 115/256\n",
            "Processed batch 116/256\n",
            "Processed batch 117/256\n",
            "Processed batch 118/256\n",
            "Processed batch 119/256\n",
            "Processed batch 120/256\n",
            "Processed batch 121/256\n",
            "Processed batch 122/256\n",
            "Processed batch 123/256\n",
            "Processed batch 124/256\n",
            "Processed batch 125/256\n",
            "Processed batch 126/256\n",
            "Processed batch 127/256\n",
            "Processed batch 128/256\n",
            "Processed batch 129/256\n",
            "Processed batch 130/256\n",
            "Processed batch 131/256\n",
            "Processed batch 132/256\n",
            "Processed batch 133/256\n",
            "Processed batch 134/256\n",
            "Processed batch 135/256\n",
            "Processed batch 136/256\n",
            "Processed batch 137/256\n",
            "Processed batch 138/256\n",
            "Processed batch 139/256\n",
            "Processed batch 140/256\n",
            "Processed batch 141/256\n",
            "Processed batch 142/256\n",
            "Processed batch 143/256\n",
            "Processed batch 144/256\n",
            "Processed batch 145/256\n",
            "Processed batch 146/256\n",
            "Processed batch 147/256\n",
            "Processed batch 148/256\n",
            "Processed batch 149/256\n",
            "Processed batch 150/256\n",
            "Processed batch 151/256\n",
            "Processed batch 152/256\n",
            "Processed batch 153/256\n",
            "Processed batch 154/256\n",
            "Processed batch 155/256\n",
            "Processed batch 156/256\n",
            "Processed batch 157/256\n",
            "Processed batch 158/256\n",
            "Processed batch 159/256\n",
            "Processed batch 160/256\n",
            "Processed batch 161/256\n",
            "Processed batch 162/256\n",
            "Processed batch 163/256\n",
            "Processed batch 164/256\n",
            "Processed batch 165/256\n",
            "Processed batch 166/256\n",
            "Processed batch 167/256\n",
            "Processed batch 168/256\n",
            "Processed batch 169/256\n",
            "Processed batch 170/256\n",
            "Processed batch 171/256\n",
            "Processed batch 172/256\n",
            "Processed batch 173/256\n",
            "Processed batch 174/256\n",
            "Processed batch 175/256\n",
            "Processed batch 176/256\n",
            "Processed batch 177/256\n",
            "Processed batch 178/256\n",
            "Processed batch 179/256\n",
            "Processed batch 180/256\n",
            "Processed batch 181/256\n",
            "Processed batch 182/256\n",
            "Processed batch 183/256\n",
            "Processed batch 184/256\n",
            "Processed batch 185/256\n",
            "Processed batch 186/256\n",
            "Processed batch 187/256\n",
            "Processed batch 188/256\n",
            "Processed batch 189/256\n",
            "Processed batch 190/256\n",
            "Processed batch 191/256\n",
            "Processed batch 192/256\n",
            "Processed batch 193/256\n",
            "Processed batch 194/256\n",
            "Processed batch 195/256\n",
            "Processed batch 196/256\n",
            "Processed batch 197/256\n",
            "Processed batch 198/256\n",
            "Processed batch 199/256\n",
            "Processed batch 200/256\n",
            "Processed batch 201/256\n",
            "Processed batch 202/256\n",
            "Processed batch 203/256\n",
            "Processed batch 204/256\n",
            "Processed batch 205/256\n",
            "Processed batch 206/256\n",
            "Processed batch 207/256\n",
            "Processed batch 208/256\n",
            "Processed batch 209/256\n",
            "Processed batch 210/256\n",
            "Processed batch 211/256\n",
            "Processed batch 212/256\n",
            "Processed batch 213/256\n",
            "Processed batch 214/256\n",
            "Processed batch 215/256\n",
            "Processed batch 216/256\n",
            "Processed batch 217/256\n",
            "Processed batch 218/256\n",
            "Processed batch 219/256\n",
            "Processed batch 220/256\n",
            "Processed batch 221/256\n",
            "Processed batch 222/256\n",
            "Processed batch 223/256\n",
            "Processed batch 224/256\n",
            "Processed batch 225/256\n",
            "Processed batch 226/256\n",
            "Processed batch 227/256\n",
            "Processed batch 228/256\n",
            "Processed batch 229/256\n",
            "Processed batch 230/256\n",
            "Processed batch 231/256\n",
            "Processed batch 232/256\n",
            "Processed batch 233/256\n",
            "Processed batch 234/256\n",
            "Processed batch 235/256\n",
            "Processed batch 236/256\n",
            "Processed batch 237/256\n",
            "Processed batch 238/256\n",
            "Processed batch 239/256\n",
            "Processed batch 240/256\n",
            "Processed batch 241/256\n",
            "Processed batch 242/256\n",
            "Processed batch 243/256\n",
            "Processed batch 244/256\n",
            "Processed batch 245/256\n",
            "Processed batch 246/256\n",
            "Processed batch 247/256\n",
            "Processed batch 248/256\n",
            "Processed batch 249/256\n",
            "Processed batch 250/256\n",
            "Processed batch 251/256\n",
            "Processed batch 252/256\n",
            "Processed batch 253/256\n",
            "Processed batch 254/256\n",
            "Processed batch 255/256\n",
            "Processed batch 256/256\n",
            "Embedding dimension: 1536\n",
            "Number of embeddings generated: 25545\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Create Unique IDs and Combine Metadata"
      ],
      "metadata": {
        "id": "HmIIq1Zh5sLZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create unique IDs for each instruction\n",
        "ids = [str(i) for i in range(len(data))]\n",
        "\n",
        "# Function to combine embeddings with metadata for Pinecone upload\n",
        "def prepare_data_for_pinecone(documents, doc_embeddings):\n",
        "    data_with_metadata = []\n",
        "    for doc_id, doc_text, embedding in zip(ids, documents, doc_embeddings):\n",
        "        # Ensure text is a string\n",
        "        doc_text = str(doc_text) if not isinstance(doc_text, str) else doc_text\n",
        "\n",
        "        # Create data item with ID, embedding, and metadata\n",
        "        data_item = {\n",
        "            \"id\": doc_id,\n",
        "            \"values\": embedding,\n",
        "            \"metadata\": {\"text\": doc_text},\n",
        "        }\n",
        "        data_with_metadata.append(data_item)\n",
        "    return data_with_metadata\n",
        "\n",
        "# Prepare the data for Pinecone upload\n",
        "all_meta_data = prepare_data_for_pinecone(data['instruction'].tolist(), data['instruction_embedding'].tolist())\n"
      ],
      "metadata": {
        "id": "S7azAdJi5t4C"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Initialize Pinecone and Create the Index"
      ],
      "metadata": {
        "id": "rWrDnTBz_hj9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pinecone import Pinecone, ServerlessSpec\n",
        "\n",
        "# Initialize Pinecone\n",
        "pinecone = Pinecone()\n",
        "\n",
        "INDEX_NAME = \"credit-card-instructions\"\n",
        "\n",
        "# Check if the index already exists and delete it if needed\n",
        "if INDEX_NAME in [index.name for index in pinecone.list_indexes()]:\n",
        "    pinecone.delete_index(INDEX_NAME)\n",
        "\n",
        "# Create the index with specified dimensions and cosine similarity metric\n",
        "pinecone.create_index(\n",
        "    name=INDEX_NAME,\n",
        "    dimension=len(data['instruction_embedding'][0]),  # Adjust dimension based on embedding size\n",
        "    metric=\"cosine\",\n",
        "    spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\")\n",
        ")\n",
        "\n",
        "# Connect to the created index\n",
        "index = pinecone.Index(INDEX_NAME)\n"
      ],
      "metadata": {
        "id": "PdJkAzRV-voz"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Upsert Embeddings and Metadata into Pinecone in Batches"
      ],
      "metadata": {
        "id": "md9IR980_sgS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to upsert data to Pinecone in batches\n",
        "def upsert_in_batches(data, batch_size=100):\n",
        "    for i in range(0, len(data), batch_size):\n",
        "        batch = data[i:i + batch_size]\n",
        "        index.upsert(vectors=batch)\n",
        "        print(f\"Uploaded batch {i // batch_size + 1} of {len(data) // batch_size + 1}\")\n",
        "\n",
        "# Upsert embeddings and metadata in batches\n",
        "upsert_in_batches(all_meta_data)\n",
        "print(\"All data successfully uploaded to Pinecone in batches.\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cy_YrHOH_r2L",
        "outputId": "c9e10ef2-d99f-44dd-d9cd-8048b461dd34"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Uploaded batch 1 of 256\n",
            "Uploaded batch 2 of 256\n",
            "Uploaded batch 3 of 256\n",
            "Uploaded batch 4 of 256\n",
            "Uploaded batch 5 of 256\n",
            "Uploaded batch 6 of 256\n",
            "Uploaded batch 7 of 256\n",
            "Uploaded batch 8 of 256\n",
            "Uploaded batch 9 of 256\n",
            "Uploaded batch 10 of 256\n",
            "Uploaded batch 11 of 256\n",
            "Uploaded batch 12 of 256\n",
            "Uploaded batch 13 of 256\n",
            "Uploaded batch 14 of 256\n",
            "Uploaded batch 15 of 256\n",
            "Uploaded batch 16 of 256\n",
            "Uploaded batch 17 of 256\n",
            "Uploaded batch 18 of 256\n",
            "Uploaded batch 19 of 256\n",
            "Uploaded batch 20 of 256\n",
            "Uploaded batch 21 of 256\n",
            "Uploaded batch 22 of 256\n",
            "Uploaded batch 23 of 256\n",
            "Uploaded batch 24 of 256\n",
            "Uploaded batch 25 of 256\n",
            "Uploaded batch 26 of 256\n",
            "Uploaded batch 27 of 256\n",
            "Uploaded batch 28 of 256\n",
            "Uploaded batch 29 of 256\n",
            "Uploaded batch 30 of 256\n",
            "Uploaded batch 31 of 256\n",
            "Uploaded batch 32 of 256\n",
            "Uploaded batch 33 of 256\n",
            "Uploaded batch 34 of 256\n",
            "Uploaded batch 35 of 256\n",
            "Uploaded batch 36 of 256\n",
            "Uploaded batch 37 of 256\n",
            "Uploaded batch 38 of 256\n",
            "Uploaded batch 39 of 256\n",
            "Uploaded batch 40 of 256\n",
            "Uploaded batch 41 of 256\n",
            "Uploaded batch 42 of 256\n",
            "Uploaded batch 43 of 256\n",
            "Uploaded batch 44 of 256\n",
            "Uploaded batch 45 of 256\n",
            "Uploaded batch 46 of 256\n",
            "Uploaded batch 47 of 256\n",
            "Uploaded batch 48 of 256\n",
            "Uploaded batch 49 of 256\n",
            "Uploaded batch 50 of 256\n",
            "Uploaded batch 51 of 256\n",
            "Uploaded batch 52 of 256\n",
            "Uploaded batch 53 of 256\n",
            "Uploaded batch 54 of 256\n",
            "Uploaded batch 55 of 256\n",
            "Uploaded batch 56 of 256\n",
            "Uploaded batch 57 of 256\n",
            "Uploaded batch 58 of 256\n",
            "Uploaded batch 59 of 256\n",
            "Uploaded batch 60 of 256\n",
            "Uploaded batch 61 of 256\n",
            "Uploaded batch 62 of 256\n",
            "Uploaded batch 63 of 256\n",
            "Uploaded batch 64 of 256\n",
            "Uploaded batch 65 of 256\n",
            "Uploaded batch 66 of 256\n",
            "Uploaded batch 67 of 256\n",
            "Uploaded batch 68 of 256\n",
            "Uploaded batch 69 of 256\n",
            "Uploaded batch 70 of 256\n",
            "Uploaded batch 71 of 256\n",
            "Uploaded batch 72 of 256\n",
            "Uploaded batch 73 of 256\n",
            "Uploaded batch 74 of 256\n",
            "Uploaded batch 75 of 256\n",
            "Uploaded batch 76 of 256\n",
            "Uploaded batch 77 of 256\n",
            "Uploaded batch 78 of 256\n",
            "Uploaded batch 79 of 256\n",
            "Uploaded batch 80 of 256\n",
            "Uploaded batch 81 of 256\n",
            "Uploaded batch 82 of 256\n",
            "Uploaded batch 83 of 256\n",
            "Uploaded batch 84 of 256\n",
            "Uploaded batch 85 of 256\n",
            "Uploaded batch 86 of 256\n",
            "Uploaded batch 87 of 256\n",
            "Uploaded batch 88 of 256\n",
            "Uploaded batch 89 of 256\n",
            "Uploaded batch 90 of 256\n",
            "Uploaded batch 91 of 256\n",
            "Uploaded batch 92 of 256\n",
            "Uploaded batch 93 of 256\n",
            "Uploaded batch 94 of 256\n",
            "Uploaded batch 95 of 256\n",
            "Uploaded batch 96 of 256\n",
            "Uploaded batch 97 of 256\n",
            "Uploaded batch 98 of 256\n",
            "Uploaded batch 99 of 256\n",
            "Uploaded batch 100 of 256\n",
            "Uploaded batch 101 of 256\n",
            "Uploaded batch 102 of 256\n",
            "Uploaded batch 103 of 256\n",
            "Uploaded batch 104 of 256\n",
            "Uploaded batch 105 of 256\n",
            "Uploaded batch 106 of 256\n",
            "Uploaded batch 107 of 256\n",
            "Uploaded batch 108 of 256\n",
            "Uploaded batch 109 of 256\n",
            "Uploaded batch 110 of 256\n",
            "Uploaded batch 111 of 256\n",
            "Uploaded batch 112 of 256\n",
            "Uploaded batch 113 of 256\n",
            "Uploaded batch 114 of 256\n",
            "Uploaded batch 115 of 256\n",
            "Uploaded batch 116 of 256\n",
            "Uploaded batch 117 of 256\n",
            "Uploaded batch 118 of 256\n",
            "Uploaded batch 119 of 256\n",
            "Uploaded batch 120 of 256\n",
            "Uploaded batch 121 of 256\n",
            "Uploaded batch 122 of 256\n",
            "Uploaded batch 123 of 256\n",
            "Uploaded batch 124 of 256\n",
            "Uploaded batch 125 of 256\n",
            "Uploaded batch 126 of 256\n",
            "Uploaded batch 127 of 256\n",
            "Uploaded batch 128 of 256\n",
            "Uploaded batch 129 of 256\n",
            "Uploaded batch 130 of 256\n",
            "Uploaded batch 131 of 256\n",
            "Uploaded batch 132 of 256\n",
            "Uploaded batch 133 of 256\n",
            "Uploaded batch 134 of 256\n",
            "Uploaded batch 135 of 256\n",
            "Uploaded batch 136 of 256\n",
            "Uploaded batch 137 of 256\n",
            "Uploaded batch 138 of 256\n",
            "Uploaded batch 139 of 256\n",
            "Uploaded batch 140 of 256\n",
            "Uploaded batch 141 of 256\n",
            "Uploaded batch 142 of 256\n",
            "Uploaded batch 143 of 256\n",
            "Uploaded batch 144 of 256\n",
            "Uploaded batch 145 of 256\n",
            "Uploaded batch 146 of 256\n",
            "Uploaded batch 147 of 256\n",
            "Uploaded batch 148 of 256\n",
            "Uploaded batch 149 of 256\n",
            "Uploaded batch 150 of 256\n",
            "Uploaded batch 151 of 256\n",
            "Uploaded batch 152 of 256\n",
            "Uploaded batch 153 of 256\n",
            "Uploaded batch 154 of 256\n",
            "Uploaded batch 155 of 256\n",
            "Uploaded batch 156 of 256\n",
            "Uploaded batch 157 of 256\n",
            "Uploaded batch 158 of 256\n",
            "Uploaded batch 159 of 256\n",
            "Uploaded batch 160 of 256\n",
            "Uploaded batch 161 of 256\n",
            "Uploaded batch 162 of 256\n",
            "Uploaded batch 163 of 256\n",
            "Uploaded batch 164 of 256\n",
            "Uploaded batch 165 of 256\n",
            "Uploaded batch 166 of 256\n",
            "Uploaded batch 167 of 256\n",
            "Uploaded batch 168 of 256\n",
            "Uploaded batch 169 of 256\n",
            "Uploaded batch 170 of 256\n",
            "Uploaded batch 171 of 256\n",
            "Uploaded batch 172 of 256\n",
            "Uploaded batch 173 of 256\n",
            "Uploaded batch 174 of 256\n",
            "Uploaded batch 175 of 256\n",
            "Uploaded batch 176 of 256\n",
            "Uploaded batch 177 of 256\n",
            "Uploaded batch 178 of 256\n",
            "Uploaded batch 179 of 256\n",
            "Uploaded batch 180 of 256\n",
            "Uploaded batch 181 of 256\n",
            "Uploaded batch 182 of 256\n",
            "Uploaded batch 183 of 256\n",
            "Uploaded batch 184 of 256\n",
            "Uploaded batch 185 of 256\n",
            "Uploaded batch 186 of 256\n",
            "Uploaded batch 187 of 256\n",
            "Uploaded batch 188 of 256\n",
            "Uploaded batch 189 of 256\n",
            "Uploaded batch 190 of 256\n",
            "Uploaded batch 191 of 256\n",
            "Uploaded batch 192 of 256\n",
            "Uploaded batch 193 of 256\n",
            "Uploaded batch 194 of 256\n",
            "Uploaded batch 195 of 256\n",
            "Uploaded batch 196 of 256\n",
            "Uploaded batch 197 of 256\n",
            "Uploaded batch 198 of 256\n",
            "Uploaded batch 199 of 256\n",
            "Uploaded batch 200 of 256\n",
            "Uploaded batch 201 of 256\n",
            "Uploaded batch 202 of 256\n",
            "Uploaded batch 203 of 256\n",
            "Uploaded batch 204 of 256\n",
            "Uploaded batch 205 of 256\n",
            "Uploaded batch 206 of 256\n",
            "Uploaded batch 207 of 256\n",
            "Uploaded batch 208 of 256\n",
            "Uploaded batch 209 of 256\n",
            "Uploaded batch 210 of 256\n",
            "Uploaded batch 211 of 256\n",
            "Uploaded batch 212 of 256\n",
            "Uploaded batch 213 of 256\n",
            "Uploaded batch 214 of 256\n",
            "Uploaded batch 215 of 256\n",
            "Uploaded batch 216 of 256\n",
            "Uploaded batch 217 of 256\n",
            "Uploaded batch 218 of 256\n",
            "Uploaded batch 219 of 256\n",
            "Uploaded batch 220 of 256\n",
            "Uploaded batch 221 of 256\n",
            "Uploaded batch 222 of 256\n",
            "Uploaded batch 223 of 256\n",
            "Uploaded batch 224 of 256\n",
            "Uploaded batch 225 of 256\n",
            "Uploaded batch 226 of 256\n",
            "Uploaded batch 227 of 256\n",
            "Uploaded batch 228 of 256\n",
            "Uploaded batch 229 of 256\n",
            "Uploaded batch 230 of 256\n",
            "Uploaded batch 231 of 256\n",
            "Uploaded batch 232 of 256\n",
            "Uploaded batch 233 of 256\n",
            "Uploaded batch 234 of 256\n",
            "Uploaded batch 235 of 256\n",
            "Uploaded batch 236 of 256\n",
            "Uploaded batch 237 of 256\n",
            "Uploaded batch 238 of 256\n",
            "Uploaded batch 239 of 256\n",
            "Uploaded batch 240 of 256\n",
            "Uploaded batch 241 of 256\n",
            "Uploaded batch 242 of 256\n",
            "Uploaded batch 243 of 256\n",
            "Uploaded batch 244 of 256\n",
            "Uploaded batch 245 of 256\n",
            "Uploaded batch 246 of 256\n",
            "Uploaded batch 247 of 256\n",
            "Uploaded batch 248 of 256\n",
            "Uploaded batch 249 of 256\n",
            "Uploaded batch 250 of 256\n",
            "Uploaded batch 251 of 256\n",
            "Uploaded batch 252 of 256\n",
            "Uploaded batch 253 of 256\n",
            "Uploaded batch 254 of 256\n",
            "Uploaded batch 255 of 256\n",
            "Uploaded batch 256 of 256\n",
            "All data successfully uploaded to Pinecone in batches.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Generate Query Embedding and Search Pinecone"
      ],
      "metadata": {
        "id": "1T0eYdWZCSQ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def query_pinecone_for_relevant_docs(query, top_k=3):\n",
        "    \"\"\"\n",
        "    Embed the user query and retrieve relevant documents from Pinecone.\n",
        "\n",
        "    Args:\n",
        "    - query (str): The user’s question.\n",
        "    - top_k (int): Number of top matches to retrieve from Pinecone.\n",
        "\n",
        "    Returns:\n",
        "    - list: A list of tuples containing similarity scores and document texts.\n",
        "    \"\"\"\n",
        "    # Generate the query embedding\n",
        "    query_embedding = embed_model.embed_query(query)\n",
        "\n",
        "    # Query Pinecone index with the query embedding\n",
        "    query_response = index.query(vector=query_embedding, top_k=top_k, include_metadata=True)\n",
        "\n",
        "    # Extract similarities and document texts\n",
        "    results = [(match['score'], match['metadata']['text']) for match in query_response['matches']]\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "r5VgOrnUDLBo"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Filter Results for Relevance"
      ],
      "metadata": {
        "id": "6dGXkIIPCy8S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_relevance(results, threshold=0.8):\n",
        "    \"\"\"\n",
        "    Filter the results based on a similarity threshold to ensure relevance.\n",
        "\n",
        "    Args:\n",
        "    - results (list): A list of tuples with similarity scores and document texts.\n",
        "    - threshold (float): Minimum similarity score to be considered relevant.\n",
        "\n",
        "    Returns:\n",
        "    - str: Relevant document text or a message indicating off-topic content.\n",
        "    \"\"\"\n",
        "    # Check if any result meets the relevance threshold\n",
        "    for score, document in results:\n",
        "        if score >= threshold:\n",
        "            return document\n",
        "\n",
        "    # Return a message if no document meets the threshold\n",
        "    return \"The question is beyond the scope of the current documents.\"\n"
      ],
      "metadata": {
        "id": "DNBauTVvCW9e"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Use LLM for Final Response"
      ],
      "metadata": {
        "id": "Yvvrp7PsDgoY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# Initialize the LLM\n",
        "LLM = ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo\")\n",
        "\n",
        "def generate_llm_response(retrieved_document, query):\n",
        "    \"\"\"\n",
        "    Generate a final response using the LLM.\n",
        "\n",
        "    Args:\n",
        "    - retrieved_document (str): The relevant document retrieved from Pinecone.\n",
        "    - query (str): The original user query.\n",
        "\n",
        "    Returns:\n",
        "    - str: The final response from the LLM.\n",
        "    \"\"\"\n",
        "    prompt = f\"{retrieved_document} Using the provided information, answer the following question: {query}\"\n",
        "    answer = LLM.invoke(prompt)\n",
        "    return answer.content\n"
      ],
      "metadata": {
        "id": "La-9kGNMDjt2"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##To streamline the workflow, let’s add the main function, handle_user_query, which will tie everything together."
      ],
      "metadata": {
        "id": "Hb4j-BgtFTkZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def handle_user_query(query, threshold=0.8, top_k=3):\n",
        "    \"\"\"\n",
        "    Process a user query by embedding it, retrieving relevant documents, and generating a response.\n",
        "\n",
        "    Args:\n",
        "    - query (str): User’s question.\n",
        "    - threshold (float): Minimum similarity score for relevance.\n",
        "    - top_k (int): Number of top results to retrieve from Pinecone.\n",
        "\n",
        "    Returns:\n",
        "    - str: The final answer from the LLM or an off-topic message.\n",
        "    \"\"\"\n",
        "    # Step 1: Retrieve relevant documents from Pinecone\n",
        "    retrieval_results = query_pinecone_for_relevant_docs(query, top_k=top_k)\n",
        "\n",
        "    # Step 2: Filter results based on relevance threshold\n",
        "    relevant_document = filter_relevance(retrieval_results, threshold=threshold)\n",
        "\n",
        "    # Step 3: Generate response if relevant; otherwise, return off-topic message\n",
        "    if relevant_document == \"The question is beyond the scope of the current documents.\":\n",
        "        return relevant_document\n",
        "    else:\n",
        "        return generate_llm_response(relevant_document, query)\n"
      ],
      "metadata": {
        "id": "I3NS3ce1FYi8"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Query1 (Out of scope for our chatbot)\n",
        "\"Can you teach me how to drive a truck?\""
      ],
      "metadata": {
        "id": "89frq8gBEomx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example query\n",
        "query1 = \"Can you teach me how to drive a truck?\"\n",
        "\n",
        "# Step 1: Get the response from the red teaming function to check relevance\n",
        "response = handle_user_query(query1)  # This will handle retrieval and relevance filtering\n",
        "\n",
        "# Initialize the LLM\n",
        "from langchain_openai import ChatOpenAI\n",
        "LLM = ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo\")\n",
        "\n",
        "# Step 2: Create the LLM prompt based on the response\n",
        "if response == \"The question is beyond the scope of the current documents.\":\n",
        "    print(\"I'm sorry, but I don't have information on that topic. Please try asking about something related to credit card services.\")\n",
        "else:\n",
        "    # Construct prompt using the relevant document and original query\n",
        "    prompt = f\"{response} Using the provided information, answer the following question: {query1}\"\n",
        "\n",
        "    # Step 3: Generate the final answer with LLM\n",
        "    def query_response(prompt):\n",
        "        answer = LLM.invoke(prompt)\n",
        "        return answer.content\n",
        "\n",
        "    # Get and print the final answer\n",
        "    final_answer = query_response(prompt=prompt)\n",
        "    print(f\"\\nResponse from the LLM:\\n{final_answer}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RV1_kftaD7wd",
        "outputId": "339db43d-d68e-4e71-e491-42b76078247d"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I'm sorry, but I don't have information on that topic. Please try asking about something related to credit card services.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Query2\n",
        "Can you help me in activating the credit card?"
      ],
      "metadata": {
        "id": "YYEKqCkKK0ao"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example query\n",
        "query2 = \"Can you help me in activating the credit card?\"\n",
        "\n",
        "# Step 1: Get the response from the red teaming function to check relevance\n",
        "response = handle_user_query(query2)  # This will handle retrieval and relevance filtering\n",
        "\n",
        "# Initialize the LLM\n",
        "from langchain_openai import ChatOpenAI\n",
        "LLM = ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo\")\n",
        "\n",
        "# Step 2: Create the LLM prompt based on the response\n",
        "if response == \"The question is beyond the scope of the current documents.\":\n",
        "    print(\"I'm sorry, but I don't have information on that topic. Please try asking about something related to credit card services.\")\n",
        "else:\n",
        "    # Construct prompt using the relevant document and original query\n",
        "    prompt = f\"{response} Using the provided information, answer the following question: {query2}\"\n",
        "\n",
        "    # Step 3: Generate the final answer with LLM\n",
        "    def query_response(prompt):\n",
        "        answer = LLM.invoke(prompt)\n",
        "        return answer.content\n",
        "\n",
        "    # Get and print the final answer\n",
        "    final_answer = query_response(prompt=prompt)\n",
        "    print(f\"\\nResponse from the LLM:\\n{final_answer}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m02LV2g8JYsF",
        "outputId": "295daf87-fdc1-4822-a279-3ba6dab2a3a0"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Response from the LLM:\n",
            "Yes, I can help you activate your credit card. Please provide me with the necessary information and I will guide you through the activation process.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0pmMW8BTLaNO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Query3\n",
        "\n",
        "I'd like information about my recent transactions were can i find it"
      ],
      "metadata": {
        "id": "ARGRSAxKLMjh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example query\n",
        "query3 = \"I'd like information about my recent transactions were can i find it\"\n",
        "\n",
        "# Step 1: Get the response from the red teaming function to check relevance\n",
        "response = handle_user_query(query3)  # This will handle retrieval and relevance filtering\n",
        "\n",
        "# Initialize the LLM\n",
        "from langchain_openai import ChatOpenAI\n",
        "LLM = ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo\")\n",
        "\n",
        "# Step 2: Create the LLM prompt based on the response\n",
        "if response == \"The question is beyond the scope of the current documents.\":\n",
        "    print(\"I'm sorry, but I don't have information on that topic. Please try asking about something related to credit card services.\")\n",
        "else:\n",
        "    # Construct prompt using the relevant document and original query\n",
        "    prompt = f\"{response} Using the provided information, answer the following question: {query3}\"\n",
        "\n",
        "    # Step 3: Generate the final answer with LLM\n",
        "    def query_response(prompt):\n",
        "        answer = LLM.invoke(prompt)\n",
        "        return answer.content\n",
        "\n",
        "    # Get and print the final answer\n",
        "    final_answer = query_response(prompt=prompt)\n",
        "    print(f\"\\nResponse from the LLM:\\n{final_answer}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6DG7NAsdLcDH",
        "outputId": "257482a9-7e81-4cd5-dc20-cfe7b00e1787"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Response from the LLM:\n",
            "You can find information about your recent transactions by checking your bank statement online or through your mobile banking app. You can also contact your bank directly for assistance in accessing this information.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Query 4\n",
        "I need to recover a Visa by an ATM, where do I do it?"
      ],
      "metadata": {
        "id": "B7BPDXXuMV8c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example query\n",
        "query4 = \"How to pay the card bill?\"\n",
        "\n",
        "# Step 1: Get the response from the red teaming function to check relevance\n",
        "response = handle_user_query(query4)  # This will handle retrieval and relevance filtering\n",
        "\n",
        "# Initialize the LLM\n",
        "from langchain_openai import ChatOpenAI\n",
        "LLM = ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo\")\n",
        "\n",
        "# Step 2: Create the LLM prompt based on the response\n",
        "if response == \"The question is beyond the scope of the current documents.\":\n",
        "    print(\"I'm sorry, but I don't have information on that topic. Please try asking about something related to credit card services.\")\n",
        "else:\n",
        "    # Construct prompt using the relevant document and original query\n",
        "    prompt = f\"{response} Using the provided information, answer the following question: {query4}\"\n",
        "\n",
        "    # Step 3: Generate the final answer with LLM\n",
        "    def query_response(prompt):\n",
        "        answer = LLM.invoke(prompt)\n",
        "        return answer.content\n",
        "\n",
        "    # Get and print the final answer\n",
        "    final_answer = query_response(prompt=prompt)\n",
        "    print(f\"\\nResponse from the LLM:\\n{final_answer}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UeQLgh13Mcgw",
        "outputId": "a39bed99-42fe-4b30-ad04-acb1d486a035"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Response from the LLM:\n",
            "You can pay the card bill by making a payment online through the credit card issuer's website, setting up automatic payments, paying by phone, mailing a check, or visiting a branch of the issuing bank.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Query6\n",
        "Who won the fifa worldcup last year?"
      ],
      "metadata": {
        "id": "o6q4tKgjPab9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example query\n",
        "query6 = \"Who won the fifa worldcup last year?\"\n",
        "\n",
        "# Step 1: Get the response from the red teaming function to check relevance\n",
        "response = handle_user_query(query6)  # This will handle retrieval and relevance filtering\n",
        "\n",
        "# Initialize the LLM\n",
        "from langchain_openai import ChatOpenAI\n",
        "LLM = ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo\")\n",
        "\n",
        "# Step 2: Create the LLM prompt based on the response\n",
        "if response == \"The question is beyond the scope of the current documents.\":\n",
        "    print(\"I'm sorry, but I don't have information on that topic. Please try asking about something related to credit card services.\")\n",
        "else:\n",
        "    # Construct prompt using the relevant document and original query\n",
        "    prompt = f\"{response} Using the provided information, answer the following question: {query6}\"\n",
        "\n",
        "    # Step 3: Generate the final answer with LLM\n",
        "    def query_response(prompt):\n",
        "        answer = LLM.invoke(prompt)\n",
        "        return answer.content\n",
        "\n",
        "    # Get and print the final answer\n",
        "    final_answer = query_response(prompt=prompt)\n",
        "    print(f\"\\nResponse from the LLM:\\n{final_answer}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pioha94zO-Dx",
        "outputId": "605474d1-4673-45b1-a565-bb7d16181075"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I'm sorry, but I don't have information on that topic. Please try asking about something related to credit card services.\n"
          ]
        }
      ]
    }
  ]
}